---
title: "datatidy"
author: "Ying-Jung Deweese"
date: "January 16, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning = FALSE,message = FALSE}
library(dplyr)
library(tidyr)

```

to call a function from a specific package 'package_name::function_name(...)'


#Data Cleaning

Read in data file

```{r}
catch_df<- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1", method = "libcurl"),
                    stringsAsFactors = FALSE)
head(catch_df)
```

ctrl+shift+M %>% pipe opearator

*Remove marginal sum and note column
*Move from wide to long format
```{r}
catch_long<-catch_df %>% 
  #negative value remove columns
  select(-All,-notesRegCode) %>% 
  gather(key="species",value = "catch",-Year,-Region)

head(catch_long)


```

*erroneous value due to OCR issue  change "I" to one
*crate catch column in correct units


```{r}
catch_clean<-catch_long %>% 
  rename(catch_thousands=catch) %>%
  mutate(catch_thousands = ifelse(catch_thousands =="I","1",catch_thousands)) %>% 
  mutate(catch_thousands = as.integer(catch_thousands))%>% 
  mutate(catch = catch_thousands*as.integer(1000))

tail(catch_clean)

```

```{r,eval=FALSE,echo=FALSE}

# I use this code to find the bad value

test_catch<-as.integer(catch_clean$catch_thousands)

i<-which(is.na(test_catch) == T)

catch_clean[i,]

```


#Split-Apply-Combine

Calculate total catch by region

```{r}

catch_total<-catch_clean %>% 
  group_by(Region, Year) %>% 
  summarize(catch_region=sum(catch),
            n_obs=n())# # of rows went in the sume of catch

catch_total

```

```{r}
catch_matt<-catch_clean %>% 
  group_by(species,Year) %>% 
  summarize(catch_mean=mean(catch), # calculation mean
            catch_sd=sd(catch),
            catch_n=n())# total_obs per group

head(catch_matt)
```

Filter for chinook salmon

```{r}
catch_chinook<-catch_clean %>% 
  #filter(species=="Chinook"| species=="Chum")
  filter(species=="Chinook" & Region =="SSE" & Year>1990) %>% 
  arrange(-Year)

head(catch_chinook)

```


#Joins

Read in region definitions files

```{r}
region_defs <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1", method = "libcurl"),
                        stringsAsFactors = FALSE)



head(region_defs)

```

Clean up the input data

```{r}
region_clean<-region_defs %>% 
  select(code,mgmtArea) %>% 
  rename(Region=code)

head(region_clean)
```

```{r}
catch_joined<- left_join(catch_clean,region_clean)
                         #by=c("Region"="code")

head(catch_joined)
  
```

#Spread

Make a wide dataframe using spread

```{r}
catch_wide<-catch_clean %>% 
  filter(Year>1990) %>% 
  select(-catch_thousands) %>% 
  spread(key=Year, value = catch )

head(catch_wide)



```

#Separate

YYYY-MM-DD

```{r}
dates_df <- data.frame(date = c("5/24/1930",
                                "5/25/1930",
                                "5/26/1930",
                                "5/27/1930",
                                "5/28/1930"),
                       stringsAsFactors = FALSE)

dates_sep<-dates_df %>% 
  separate(date, c("month", "day", "year"), "/", remove = F)

head(dates_sep)






```

```{r}
cities_df <- data.frame(city = c("Juneau AK", 
                                 "Sitka AK", 
                                 "Anchorage AK"),
                        stringsAsFactors = FALSE)



```


#Unite



```{r}
dates_unite<-dates_sep %>% 
  #separate(date, c("month", "day", "year"), "/") %>% 
  unite(date_iso, year, month, day, sep = "-")

head(dates_unite)
```

